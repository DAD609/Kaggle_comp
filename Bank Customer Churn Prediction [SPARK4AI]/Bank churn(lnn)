{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76741,"databundleVersionId":8346697,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gitpython\nfrom git import Repo\npath_to_clone = '/kaggle/working/lnn'\nrepo_url = 'https://github.com/kyegomez/LiqudNet.git'\nRepo.clone_from(repo_url, path_to_clone)\n!pip install -r /kaggle/working/lnn/requirements.txt\n!pip install liquidnet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-30T06:46:28.523748Z","iopub.execute_input":"2024-05-30T06:46:28.524139Z","iopub.status.idle":"2024-05-30T06:47:17.349537Z","shell.execute_reply.started":"2024-05-30T06:46:28.524107Z","shell.execute_reply":"2024-05-30T06:47:17.348294Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gitpython in /opt/conda/lib/python3.10/site-packages (3.1.41)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython) (4.0.11)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython) (5.0.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r /kaggle/working/lnn/requirements.txt (line 1)) (2.1.2+cpu)\nCollecting einops (from -r /kaggle/working/lnn/requirements.txt (line 2))\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/working/lnn/requirements.txt (line 1)) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/working/lnn/requirements.txt (line 1)) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/working/lnn/requirements.txt (line 1)) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/working/lnn/requirements.txt (line 1)) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/working/lnn/requirements.txt (line 1)) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->-r /kaggle/working/lnn/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->-r /kaggle/working/lnn/requirements.txt (line 1)) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r /kaggle/working/lnn/requirements.txt (line 1)) (1.3.0)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting liquidnet\n  Downloading liquidnet-0.0.5-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from liquidnet) (0.8.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from liquidnet) (2.1.2+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->liquidnet) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->liquidnet) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->liquidnet) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->liquidnet) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->liquidnet) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->liquidnet) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->liquidnet) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->liquidnet) (1.3.0)\nDownloading liquidnet-0.0.5-py3-none-any.whl (7.1 kB)\nInstalling collected packages: liquidnet\nSuccessfully installed liquidnet-0.0.5\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport keras_nlp\nimport keras\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer\nfrom liquidnet.main import LiquidNet\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:09:23.204777Z","iopub.execute_input":"2024-05-30T07:09:23.205305Z","iopub.status.idle":"2024-05-30T07:09:23.212508Z","shell.execute_reply.started":"2024-05-30T07:09:23.205269Z","shell.execute_reply":"2024-05-30T07:09:23.211293Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":" \n# Загрузка данных\ntrain_df = pd.read_csv('/kaggle/input/bank-customer-churn-prediction-spark4ai/train.csv')\ntest_df = pd.read_csv('/kaggle/input/bank-customer-churn-prediction-spark4ai/test.csv')\n\n# Удаление нерелевантных текстовых столбцов перед кодированием\ntrain_df = train_df.drop(columns=['Surname'])\ntest_df = test_df.drop(columns=['Surname'])\n\n# Предобработка данных с использованием one-hot encoding\ntrain_df_encoded = pd.get_dummies(train_df, columns=['Geography', 'Gender'])\ntest_df_encoded = pd.get_dummies(test_df, columns=['Geography', 'Gender'])\n\n# Удаление пропущенных значений\ntrain_df_clean = train_df_encoded.dropna()\ntest_df_clean = test_df_encoded.dropna()\n\n# Выделение признаков и целевой переменной\nX_train = train_df_clean.drop('Exited', axis=1)\ny_train = train_df_clean['Exited']\nX_test = test_df_clean  # Используйте очищенный и закодированный тестовый датафрейм\n\n# Убедитесь, что 'Exited' не существует в тестовом наборе, если это тестовый датафрейм для соревнования\n# X_test = test_df_clean.drop('Exited', axis=1) если 'Exited' существует и должен быть удален\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:25:50.344005Z","iopub.execute_input":"2024-05-30T07:25:50.345274Z","iopub.status.idle":"2024-05-30T07:25:50.442945Z","shell.execute_reply.started":"2024-05-30T07:25:50.345229Z","shell.execute_reply":"2024-05-30T07:25:50.441755Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Подготовка данных\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\ninputs = torch.tensor(X_train_scaled, dtype=torch.float32)\n\n# Для y_train, преобразуем pandas Series в NumPy массив и изменяем его форму\ny_train_np = y_train.to_numpy().reshape(-1, 1)\n\n# Создание тензора из NumPy массива\ntargets = torch.tensor(y_train_np, dtype=torch.float32)  # y_train должен быть массивом\n\n# Определение модели\nclass LiquidNetClassifier(nn.Module):\n    def __init__(self, num_units, num_classes=1):\n        super(LiquidNetClassifier, self).__init__()\n        self.ltc_cell = LiquidNet(num_units)\n        self.fc = nn.Linear(num_units, num_classes)\n\n    def forward(self, inputs, initial_state):\n        outputs, _ = self.ltc_cell(inputs, initial_state)\n        return torch.sigmoid(self.fc(outputs))\n\nmodel = LiquidNetClassifier(num_units=64)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\ncriterion = nn.BCELoss()\n\n# Обучение модели\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    \n    initial_state = torch.zeros(inputs.size(0), 64, dtype=torch.float32)  # Задаем начальное состояние\n\n    predictions = model(inputs, initial_state)\n    loss = criterion(predictions, targets)\n    \n    loss.backward()\n    optimizer.step()\n    \n ","metadata":{"execution":{"iopub.status.busy":"2024-05-30T11:28:50.191329Z","iopub.execute_input":"2024-05-30T11:28:50.192979Z","iopub.status.idle":"2024-05-30T11:28:56.712828Z","shell.execute_reply.started":"2024-05-30T11:28:50.192902Z","shell.execute_reply":"2024-05-30T11:28:56.711247Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Подготовка данных\u001b[39;00m\n\u001b[1;32m      8\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m----> 9\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[1;32m     10\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[1;32m     11\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train_scaled, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n","\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"],"ename":"NameError","evalue":"name 'X_train' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Подготовка данных (пример)\n# Подготовка данных: предполагаем, что X_train и y_train уже загружены как DataFrame и Series\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)  # Нормализация признаков\nX_test_scaled = scaler.transform(X_test)\n\n# Преобразование данных в тензоры\ninputs = torch.tensor(X_train_scaled, dtype=torch.float32)\ntargets = torch.tensor(y_train.to_numpy().reshape(-1, 1), dtype=torch.float32)  # Преобразование y_train в правильную форму\n\n# Определение модели\nclass LiquidNetClassifier(nn.Module):\n    def __init__(self, num_units):\n        super(LiquidNetClassifier, self).__init__()\n        self.ltc_cell = LiquidNet(num_units)\n        self.fc = nn.Linear(num_units, 1)  # предполагаем, что выходной слой должен предсказывать один выход (вероятность)\n\n    def forward(self, inputs, initial_state):\n        outputs, _ = self.ltc_cell(inputs, initial_state)\n        return torch.sigmoid(self.fc(outputs))  # используем sigmoid для получения вероятности\n\n# Создание экземпляра модели\nnum_units = 64\nmodel = LiquidNetClassifier(num_units)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.BCELoss()  # Binary Cross-Entropy Loss для задачи бинарной классификации\n\n# Обучение модели\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    \n    # Подготовка начального состояния\n    initial_state = torch.zeros(inputs.size(0), num_units, dtype=torch.float32)\n\n    # Прямой проход\n    predictions = model(inputs, initial_state)\n    \n    # Расчет потерь\n    loss = criterion(predictions, targets)\n    \n    # Обратный проход\n    loss.backward()\n    optimizer.step()\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T08:15:38.278412Z","iopub.execute_input":"2024-05-30T08:15:38.279452Z","iopub.status.idle":"2024-05-30T08:17:11.692254Z","shell.execute_reply.started":"2024-05-30T08:15:38.279398Z","shell.execute_reply":"2024-05-30T08:17:11.690548Z"},"trusted":true},"execution_count":33,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), num_units, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Прямой проход\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Расчет потерь\u001b[39;00m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, targets)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[33], line 19\u001b[0m, in \u001b[0;36mLiquidNetClassifier.forward\u001b[0;34m(self, inputs, initial_state)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, initial_state):\n\u001b[0;32m---> 19\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mltc_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(outputs))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/liquidnet/main.py:180\u001b[0m, in \u001b[0;36mLiquidNet.forward\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    176\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ode_step_explicit(\n\u001b[1;32m    177\u001b[0m         inputs, state, _ode_solver_unfolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ode_solver_unfolds\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solver \u001b[38;5;241m==\u001b[39m ODESolver\u001b[38;5;241m.\u001b[39mSemiImplicit:\n\u001b[0;32m--> 180\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ode_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solver \u001b[38;5;241m==\u001b[39m ODESolver\u001b[38;5;241m.\u001b[39mRungeKutta:\n\u001b[1;32m    182\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ode_step_runge_kutta(inputs, state)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/liquidnet/main.py:285\u001b[0m, in \u001b[0;36mLiquidNet._ode_step\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m    282\u001b[0m w_denominator_sensory \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(sensory_w_activation, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ode_solver_unfolds):\n\u001b[0;32m--> 285\u001b[0m     w_activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_pre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     rev_activation \u001b[38;5;241m=\u001b[39m w_activation \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merev\n\u001b[1;32m    289\u001b[0m     w_numerator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(rev_activation, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m w_numerator_sensory\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/liquidnet/main.py:377\u001b[0m, in \u001b[0;36mLiquidNet._sigmoid\u001b[0;34m(self, v_pre, mu, sigma)\u001b[0m\n\u001b[1;32m    375\u001b[0m mues \u001b[38;5;241m=\u001b[39m v_pre \u001b[38;5;241m-\u001b[39m mu\n\u001b[1;32m    376\u001b[0m x \u001b[38;5;241m=\u001b[39m sigma \u001b[38;5;241m*\u001b[39m mues\n\u001b[0;32m--> 377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    test_inputs = torch.tensor(X_test_scaled, dtype=torch.float32)\n    initial_state = torch.zeros(test_inputs.size(0), num_units)\n    test_predictions = model(test_inputs, initial_state)\n    test_predictions = test_predictions.numpy().flatten()  # Преобразование в numpy массив для сохранения в CSV\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:36:35.115832Z","iopub.execute_input":"2024-05-30T07:36:35.116238Z","iopub.status.idle":"2024-05-30T07:36:37.775497Z","shell.execute_reply.started":"2024-05-30T07:36:35.116209Z","shell.execute_reply":"2024-05-30T07:36:37.774462Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Сохранение предсказаний\nresults_df = pd.DataFrame({\n    'id': test_df_clean['id'],\n    'Exited': test_predictions\n})\nresults_df.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-30T07:57:36.502830Z","iopub.execute_input":"2024-05-30T07:57:36.503770Z","iopub.status.idle":"2024-05-30T07:57:36.543050Z","shell.execute_reply.started":"2024-05-30T07:57:36.503732Z","shell.execute_reply":"2024-05-30T07:57:36.541403Z"},"trusted":true},"execution_count":31,"outputs":[]}]}